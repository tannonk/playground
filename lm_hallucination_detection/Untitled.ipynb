{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535de0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "# The same interface can be used with custom models as well\n",
    "from fairseq.models.transformer_lm import TransformerLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de42f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unc_responses = [\n",
    "        \"<s> Vielen Dank für Ihre Bewertung. </s>\",\n",
    "        \"<s> Wir freuen uns sehr, dass Sie einen sehr schönen Aufenthalt bei uns hatten und freuen uns schon jetzt auf das nächste Wiedersehen! Viele Grüße, Ihr Team vom Berliner Fernsehturm </s>\",\n",
    "        \"<s> Sehr geerter A8627VEsusang, es freut mich zu lesen, dass wir wohnqualitativ für Ihre Familie optimal sind und hoffen Sie auch weiterhin in Adina´s in Hamburg, Frankfurt, Berlin, Kopenhagen und Budapest begrüßen zu dürfen. Bis zum nächsten Mal. Mit freundlichen Grüßen Annette Jost </s>\",\n",
    "        \"<s> Vielen Dank für Ihre Bewertung und Weiterempfehlung! Wir freuen uns, dass es Ihnen sehr gut gefallen hat und hoffen, dass Sie in Zukunft auch einmal unser zweites Restaurant - das Gourmet 1895 - besuchen. Mit Vorfreude auf ein Wiedersehen senden wir herzliche Grüße aus dem Hotel Kaiserhof Münster! </s>\",\n",
    "        \"<s> Liebe Tanja M Herzlichen Dank für das tolle Feedback und die Vergabe von 5 Sternen für unseren HEIMAT Burger und die Pommes. Wir freuen uns, wenn wir Dich schon bald wieder bei uns in der HEIMAT Küche + Bar begrüßen dürfen. Herzliche Grüße aus der Hafencity Dominique Alexander Ewerth F&B Manager 25hours Hotel HafenCity 25hours Hotel Altes Hafenamt </s>\",\n",
    "    ]\n",
    "\n",
    "cond_responses = [\n",
    "    \"Überraschung im Dortmund <endtitle> Wir hatten von Dortmund nicht allzu viel erwartet und waren umso überraschter dort ein so schönes Hotel vorzufinden. Sehr guter Service am Empfang, tolles, modernes Zimmer. Das Bad war nicht riesig aber gut durchdacht und äußerst sauber und modern. Das Restaurant ist gemütlich modern eingerichtet. Beim Frühstück gab es reichlich Auswahl, so dass Frühstücken wirklich Spaß gemacht hat. <s> Vielen Dank für Ihre Bewertung. </s>\",\n",
    "    \"Super schönes Erlebnis, ich werde es sehr positiv in Erinnerung behalten. <endtitle> Der Besuch war sehr schön und total romantisch. Wir haben knapp 95 € bezahlt, mit Eintritt und Tischreservierung lagen wir bei 145 Euro. Die Preise waren verständlicherweise etwas höher, das Essen war schön aufbereitet und auch sehr lecker, für den Preis jedoch nicht überdurchschnittlich gut. Bei der Vorspeise hätte ich mir gerne frisches Gemüse gewünscht, statt eingelegtes und das Fleisch war ein Tick zu trocken. Wer vorher shoppen war, sollte bedenken, dass man leider keine Taschen abgeben kann. Meckern auf hohem Niveau. Alles in allem gut. :) <s> Wir freuen uns sehr, dass Sie einen sehr schönen Aufenthalt bei uns hatten und freuen uns schon jetzt auf das nächste Wiedersehen! Viele Grüße, Ihr Team vom Berliner Fernsehturm </s>\",\n",
    "    \"tolles Hotel <endtitle> Das Konzept ist super, wir sind nun schon mit drei Kindern als auch allein zu Besuch in dieser Hotelkette gewesen, vor allem für die erste Variante ist es schwer, etwas wohnqualitativ vergleichbares zu finden. <s> Sehr geerter A8627VEsusang, es freut mich zu lesen, dass wir wohnqualitativ für Ihre Familie optimal sind und hoffen Sie auch weiterhin in Adina´s in Hamburg, Frankfurt, Berlin, Kopenhagen und Budapest begrüßen zu dürfen. Bis zum nächsten Mal. Mit freundlichen Grüßen Annette Jost </s>\",\n",
    "    \"schönes Ambiente <endtitle> Zum Hotel Kaiserhof zugehöriges Restaurant. Vielfältige Auswahl, sehr lecker. Preisniveau im oberen Segment. Service hellwach, nett und zuvorkommend. Einen Besuch wert. <s> Vielen Dank für Ihre Bewertung und Weiterempfehlung! Wir freuen uns, dass es Ihnen sehr gut gefallen hat und hoffen, dass Sie in Zukunft auch einmal unser zweites Restaurant - das Gourmet 1895 - besuchen. Mit Vorfreude auf ein Wiedersehen senden wir herzliche Grüße aus dem Hotel Kaiserhof Münster! </s>\",\n",
    "    \"Sehr lecker und üppig <endtitle> Der Heimat Burger ist sehr lecker, die Pommes dazu außergewöhnlich gut. Das Essen darf als Hotelgast mit aufs Zimmer genommen werden <s> Liebe Tanja M Herzlichen Dank für das tolle Feedback und die Vergabe von 5 Sternen für unseren HEIMAT Burger und die Pommes. Wir freuen uns, wenn wir Dich schon bald wieder bei uns in der HEIMAT Küche + Bar begrüßen dürfen. Herzliche Grüße aus der Hafencity Dominique Alexander Ewerth F&B Manager 25hours Hotel HafenCity 25hours Hotel Altes Hafenamt </s>\",    \n",
    "]\n",
    "\n",
    "lm_unc = TransformerLanguageModel.from_pretrained(\n",
    "    '/srv/scratch6/kew/lm_data/rrgen_de/response.sp-lm-data-bin',\n",
    "    '/srv/scratch6/kew/lm_data/rrgen_de/lm_unc/checkpoint_best.pt', \n",
    "    bpe='sentencepiece'\n",
    "    )\n",
    "\n",
    "lm_cond = TransformerLanguageModel.from_pretrained(\n",
    "    '/srv/scratch6/kew/lm_data/rrgen_de/rev_resp.sp-lm-data-bin',\n",
    "    '/srv/scratch6/kew/lm_data/rrgen_de/lm_cond/checkpoint_best.pt', \n",
    "    bpe='sentencepiece'\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f65a361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneratorHubInterface(\n",
       "  (models): ModuleList(\n",
       "    (0): TransformerLanguageModel(\n",
       "      (decoder): TransformerDecoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(11768, 64, padding_idx=1)\n",
       "        (embed_positions): SinusoidalPositionalEmbedding()\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (v_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (q_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (final_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerDecoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (v_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (q_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (final_layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (output_projection): Linear(in_features=64, out_features=11768, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_unc.eval()  # disable dropout\n",
    "lm_cond.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28be5912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREF \n",
      "SENT <s> Vielen Dank für Ihre Bewertung. </s>\n",
      "> \u001b[0;32m/home/user/kew/INSTALLS/pytorch_fairseq/fairseq/hub_utils.py\u001b[0m(165)\u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    163 \u001b[0;31m        \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    164 \u001b[0;31m        \u001b[0;31m# build generator using current args as well as any kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 165 \u001b[0;31m        \u001b[0mgen_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    166 \u001b[0;31m        \u001b[0;32mwith\u001b[0m \u001b[0mopen_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    167 \u001b[0;31m            \u001b[0mgen_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "> \u001b[0;32m/home/user/kew/INSTALLS/pytorch_fairseq/fairseq/tasks/language_modeling.py\u001b[0m(314)\u001b[0;36minference_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    312 \u001b[0;31m    ):\n",
      "\u001b[0m\u001b[0;32m    313 \u001b[0;31m        \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 314 \u001b[0;31m        \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    315 \u001b[0;31m            \u001b[0;31m# Generation will always be conditioned on bos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    316 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"add_bos_token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m/home/user/kew/INSTALLS/pytorch_fairseq/fairseq/tasks/language_modeling.py\u001b[0m(316)\u001b[0;36minference_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    314 \u001b[0;31m        \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    315 \u001b[0;31m            \u001b[0;31m# Generation will always be conditioned on bos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 316 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"add_bos_token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    317 \u001b[0;31m                \u001b[0mbos_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    318 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> p getattr(self.args, \"add_bos_token\", False)\n",
      "False\n",
      "ipdb> n\n",
      "> \u001b[0;32m/home/user/kew/INSTALLS/pytorch_fairseq/fairseq/tasks/language_modeling.py\u001b[0m(319)\u001b[0;36minference_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    317 \u001b[0;31m                \u001b[0mbos_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    318 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 319 \u001b[0;31m                \u001b[0mbos_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    320 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    321 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mconstraints\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> p self.source_dictionary.eos()\n",
      "2\n",
      "ipdb> n\n",
      "> \u001b[0;32m/home/user/kew/INSTALLS/pytorch_fairseq/fairseq/tasks/language_modeling.py\u001b[0m(321)\u001b[0;36minference_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    319 \u001b[0;31m                \u001b[0mbos_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    320 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 321 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mconstraints\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    322 \u001b[0;31m                raise NotImplementedError(\n",
      "\u001b[0m\u001b[0;32m    323 \u001b[0;31m                    \u001b[0;34m\"Constrained decoding with the language_modeling task is not supported\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m/home/user/kew/INSTALLS/pytorch_fairseq/fairseq/tasks/language_modeling.py\u001b[0m(326)\u001b[0;36minference_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    324 \u001b[0;31m                )\n",
      "\u001b[0m\u001b[0;32m    325 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 326 \u001b[0;31m            \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    327 \u001b[0;31m            \u001b[0;31m# SequenceGenerator doesn't use src_tokens directly, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    328 \u001b[0;31m            \u001b[0;31m# pass the `prefix_tokens` argument instead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> p prefix_tokens\n",
      "None\n",
      "ipdb> p sample[\"net_input\"][\"src_tokens\"]\n",
      "tensor([[   2,    7, 2532,   85, 6456,   70,   18,   12,   20,   31,    5,    7,\n",
      "         2532,  225,   85, 6456]])\n",
      "ipdb> c\n",
      "> \u001b[0;32m/home/user/kew/INSTALLS/pytorch_fairseq/fairseq/tasks/language_modeling.py\u001b[0m(329)\u001b[0;36minference_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    327 \u001b[0;31m            \u001b[0;31m# SequenceGenerator doesn't use src_tokens directly, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    328 \u001b[0;31m            \u001b[0;31m# pass the `prefix_tokens` argument instead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 329 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mprefix_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"net_input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"src_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    330 \u001b[0;31m                \u001b[0mprefix_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"net_input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"src_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    331 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mprefix_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbos_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "tensor([-5.6135e-03, -1.6915e+01, -8.8452e+00, -9.2386e+00, -1.5170e+00,\n",
      "        -2.9093e-01, -2.7584e-01, -1.7065e+00, -1.6321e+00, -8.2645e-01,\n",
      "        -3.7785e+00, -2.7465e+00, -1.0792e+01, -5.6704e+00, -9.4821e+00])\n",
      "PREF Überraschung im Dortmund <endtitle> Wir hatten von Dortmund nicht allzu viel erwartet und waren umso überraschter dort ein so schönes Hotel vorzufinden. Sehr guter Service am Empfang, tolles, modernes Zimmer. Das Bad war nicht riesig aber gut durchdacht und äußerst sauber und modern. Das Restaurant ist gemütlich modern eingerichtet. Beim Frühstück gab es reichlich Auswahl, so dass Frühstücken wirklich Spaß gemacht hat. \n",
      "SENT <s> Vielen Dank für Ihre Bewertung. </s>\n",
      "> \u001b[0;32m/home/user/kew/INSTALLS/pytorch_fairseq/fairseq/hub_utils.py\u001b[0m(165)\u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    163 \u001b[0;31m        \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    164 \u001b[0;31m        \u001b[0;31m# build generator using current args as well as any kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 165 \u001b[0;31m        \u001b[0mgen_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    166 \u001b[0;31m        \u001b[0;32mwith\u001b[0m \u001b[0mopen_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    167 \u001b[0;31m            \u001b[0mgen_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m/home/user/kew/INSTALLS/pytorch_fairseq/fairseq/hub_utils.py\u001b[0m(166)\u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    164 \u001b[0;31m        \u001b[0;31m# build generator using current args as well as any kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    165 \u001b[0;31m        \u001b[0mgen_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 166 \u001b[0;31m        \u001b[0;32mwith\u001b[0m \u001b[0mopen_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    167 \u001b[0;31m            \u001b[0mgen_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    168 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> p prefix_tokens\n",
      "*** NameError: name 'prefix_tokens' is not defined\n",
      "ipdb> n\n",
      "> \u001b[0;32m/home/user/kew/INSTALLS/pytorch_fairseq/fairseq/hub_utils.py\u001b[0m(167)\u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    165 \u001b[0;31m        \u001b[0mgen_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    166 \u001b[0;31m        \u001b[0;32mwith\u001b[0m \u001b[0mopen_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 167 \u001b[0;31m            \u001b[0mgen_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    168 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    169 \u001b[0;31m                \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "> \u001b[0;32m/home/user/kew/INSTALLS/pytorch_fairseq/fairseq/tasks/language_modeling.py\u001b[0m(314)\u001b[0;36minference_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    312 \u001b[0;31m    ):\n",
      "\u001b[0m\u001b[0;32m    313 \u001b[0;31m        \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 314 \u001b[0;31m        \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    315 \u001b[0;31m            \u001b[0;31m# Generation will always be conditioned on bos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    316 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"add_bos_token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> p prefix_tokens\n",
      "None\n",
      "ipdb> c\n",
      "> \u001b[0;32m/home/user/kew/INSTALLS/pytorch_fairseq/fairseq/tasks/language_modeling.py\u001b[0m(329)\u001b[0;36minference_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    327 \u001b[0;31m            \u001b[0;31m# SequenceGenerator doesn't use src_tokens directly, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    328 \u001b[0;31m            \u001b[0;31m# pass the `prefix_tokens` argument instead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 329 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mprefix_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"net_input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"src_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    330 \u001b[0;31m                \u001b[0mprefix_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"net_input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"src_tokens\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    331 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mprefix_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbos_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n",
      "tensor([ -4.4360, -11.5360,  -7.1050, -10.6806,  -6.6742,  -0.5595,  -0.2983,\n",
      "         -1.3765,  -1.0794,  -0.7730,  -3.9467,  -5.0951,  -9.8105,  -7.0775,\n",
      "        -10.5326])\n"
     ]
    }
   ],
   "source": [
    "def score(lm, sentence):\n",
    "#     breakpoint()\n",
    "    pref, sentence = sentence.split('<s>')\n",
    "    sentence = '<s>' + sentence\n",
    "    print('PREF', pref)\n",
    "    print('SENT', sentence)\n",
    "    if not pref:\n",
    "        s = lm.score(sentence)['positional_scores'] #.mean().neg().exp()\n",
    "        print(s)\n",
    "    else:\n",
    "        s = lm.score(sentence, prefix_tokens=pref)['positional_scores']\n",
    "        print(s)\n",
    "    # from hugging face:\n",
    "    # for i in range(1, len(tokenize_input)+1):\n",
    "    #     tensor_input = torch.tensor([tokenize_input[:i]])\n",
    "    #     print(tensor_input)\n",
    "    #     loss=model(tensor_input, labels=tensor_input)[0]\n",
    "    #     print(np.exp(loss.detach().numpy()))\n",
    "    # return np.exp(loss.detach().numpy())\n",
    "\n",
    "# for text in unc_responses:\n",
    "# score(lm_cond, unc_responses[0])\n",
    "score(lm_unc, unc_responses[0])\n",
    "score(lm_cond, cond_responses[0])\n",
    "# score(lm_unc[0], text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153515c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: inspect why special tokens are split:\n",
    "# https://github.com/pytorch/fairseq/blob/2fd9d8a972794ba919174baf0d1828a5a4c626f3/fairseq/data/encoders/sentencepiece_bpe.py#L34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e8f4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sentencepiece as sp\n",
    "\n",
    "spm = sp.SentencePieceProcessor(model_file='/srv/scratch6/kew/lm_data/rrgen_de/rev_resp.sp-lm-data-bin/sentencepiece.bpe.model')\n",
    "\n",
    "print(spm.EncodeAsPieces('<s> Vielen Dank!</s>'))\n",
    "print(spm.EncodeAsPieces('<BOS> Vielen Dank!<EOS>'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3989923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', '<BOS>', '▁Vielen', '▁Dank', '!', '<EOS>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e2ee38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc4636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7071b2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f62ee0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
